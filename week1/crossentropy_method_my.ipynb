{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-30 19:13:01,480] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\" % (n_states, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "policy = np.array([1 / n_actions * np.ones(n_actions)] * n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray, np.matrix)\n",
    "assert np.allclose(policy, 1. / n_actions)\n",
    "assert np.allclose(np.sum(policy, axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # <pick action from policy (at random with probabilities)>\n",
    "        a = np.random.choice(n_actions, p=policy[s, :])\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # <record prev state, action and add up reward to states,actions and total_reward accordingly>\n",
    "\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s, a, r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 738 ms, sys: 43 ms, total: 781 ms\n",
      "Wall time: 827 ms\n",
      "mean reward = -85.24000\tthreshold = -1653.0\n",
      "CPU times: user 780 ms, sys: 94.8 ms, total: 875 ms\n",
      "Wall time: 811 ms\n",
      "mean reward = -105.50400\tthreshold = -1966.0\n",
      "CPU times: user 766 ms, sys: 141 ms, total: 907 ms\n",
      "Wall time: 810 ms\n",
      "mean reward = -87.98800\tthreshold = -1199.0\n",
      "CPU times: user 691 ms, sys: 133 ms, total: 824 ms\n",
      "Wall time: 729 ms\n",
      "mean reward = -75.66000\tthreshold = -744.0\n",
      "CPU times: user 713 ms, sys: 114 ms, total: 827 ms\n",
      "Wall time: 748 ms\n",
      "mean reward = -77.39600\tthreshold = -2048.0\n",
      "CPU times: user 701 ms, sys: 65.4 ms, total: 766 ms\n",
      "Wall time: 726 ms\n",
      "mean reward = -78.85600\tthreshold = -777.0\n",
      "CPU times: user 714 ms, sys: 70.1 ms, total: 784 ms\n",
      "Wall time: 757 ms\n",
      "mean reward = -80.67200\tthreshold = -1269.0\n",
      "CPU times: user 686 ms, sys: 16.2 ms, total: 702 ms\n",
      "Wall time: 982 ms\n",
      "mean reward = -70.46400\tthreshold = -1438.0\n",
      "CPU times: user 677 ms, sys: 15.8 ms, total: 693 ms\n",
      "Wall time: 856 ms\n",
      "mean reward = -66.03600\tthreshold = -538.0\n",
      "CPU times: user 671 ms, sys: 13.8 ms, total: 685 ms\n",
      "Wall time: 736 ms\n",
      "mean reward = -67.32800\tthreshold = -475.0\n",
      "CPU times: user 615 ms, sys: 86.6 ms, total: 702 ms\n",
      "Wall time: 645 ms\n",
      "mean reward = -62.16400\tthreshold = -770.0\n",
      "CPU times: user 672 ms, sys: 56.4 ms, total: 728 ms\n",
      "Wall time: 703 ms\n",
      "mean reward = -64.49600\tthreshold = -493.0\n",
      "CPU times: user 653 ms, sys: 100 ms, total: 753 ms\n",
      "Wall time: 681 ms\n",
      "mean reward = -65.99600\tthreshold = -491.0\n",
      "CPU times: user 690 ms, sys: 36.7 ms, total: 727 ms\n",
      "Wall time: 851 ms\n",
      "mean reward = -71.72400\tthreshold = -749.0\n",
      "CPU times: user 642 ms, sys: 12 ms, total: 654 ms\n",
      "Wall time: 919 ms\n",
      "mean reward = -72.10000\tthreshold = -601.0\n",
      "CPU times: user 767 ms, sys: 19.4 ms, total: 787 ms\n",
      "Wall time: 943 ms\n",
      "mean reward = -90.47200\tthreshold = -789.0\n",
      "CPU times: user 716 ms, sys: 84 ms, total: 800 ms\n",
      "Wall time: 748 ms\n",
      "mean reward = -77.04800\tthreshold = -698.0\n",
      "CPU times: user 655 ms, sys: 8.19 ms, total: 663 ms\n",
      "Wall time: 665 ms\n",
      "mean reward = -101.14400\tthreshold = -1470.0\n",
      "CPU times: user 834 ms, sys: 19.2 ms, total: 853 ms\n",
      "Wall time: 997 ms\n",
      "mean reward = -94.43200\tthreshold = -1196.0\n",
      "CPU times: user 646 ms, sys: 6.24 ms, total: 653 ms\n",
      "Wall time: 670 ms\n",
      "mean reward = -102.77200\tthreshold = -1894.0\n",
      "CPU times: user 626 ms, sys: 22.6 ms, total: 649 ms\n",
      "Wall time: 640 ms\n",
      "mean reward = -89.17200\tthreshold = -876.0\n",
      "CPU times: user 692 ms, sys: 21.4 ms, total: 713 ms\n",
      "Wall time: 706 ms\n",
      "mean reward = -109.08400\tthreshold = -1493.0\n",
      "CPU times: user 849 ms, sys: 36.2 ms, total: 885 ms\n",
      "Wall time: 977 ms\n",
      "mean reward = -110.01600\tthreshold = -1129.0\n",
      "CPU times: user 729 ms, sys: 15.1 ms, total: 744 ms\n",
      "Wall time: 744 ms\n",
      "mean reward = -103.14400\tthreshold = -1347.0\n",
      "CPU times: user 669 ms, sys: 56.5 ms, total: 726 ms\n",
      "Wall time: 688 ms\n",
      "mean reward = -80.41600\tthreshold = -614.0\n",
      "CPU times: user 821 ms, sys: 103 ms, total: 924 ms\n",
      "Wall time: 940 ms\n",
      "mean reward = -87.13600\tthreshold = -772.0\n",
      "CPU times: user 745 ms, sys: 18.2 ms, total: 763 ms\n",
      "Wall time: 782 ms\n",
      "mean reward = -92.28400\tthreshold = -1093.0\n",
      "CPU times: user 878 ms, sys: 23.9 ms, total: 902 ms\n",
      "Wall time: 1.06 s\n",
      "mean reward = -100.98400\tthreshold = -703.0\n",
      "CPU times: user 698 ms, sys: 49.7 ms, total: 748 ms\n",
      "Wall time: 717 ms\n",
      "mean reward = -91.90800\tthreshold = -609.0\n",
      "CPU times: user 769 ms, sys: 59.3 ms, total: 828 ms\n",
      "Wall time: 791 ms\n",
      "mean reward = -89.75200\tthreshold = -619.0\n",
      "CPU times: user 894 ms, sys: 131 ms, total: 1.03 s\n",
      "Wall time: 1.05 s\n",
      "mean reward = -96.35600\tthreshold = -1161.0\n",
      "CPU times: user 884 ms, sys: 22.3 ms, total: 906 ms\n",
      "Wall time: 1.01 s\n",
      "mean reward = -92.43200\tthreshold = -855.0\n",
      "CPU times: user 915 ms, sys: 20.9 ms, total: 936 ms\n",
      "Wall time: 942 ms\n",
      "mean reward = -115.57600\tthreshold = -1505.0\n",
      "CPU times: user 1.01 s, sys: 55.3 ms, total: 1.06 s\n",
      "Wall time: 1.09 s\n",
      "mean reward = -126.02000\tthreshold = -1738.0\n",
      "CPU times: user 873 ms, sys: 12.5 ms, total: 886 ms\n",
      "Wall time: 898 ms\n",
      "mean reward = -122.26000\tthreshold = -1241.0\n",
      "CPU times: user 947 ms, sys: 63.1 ms, total: 1.01 s\n",
      "Wall time: 970 ms\n",
      "mean reward = -144.89600\tthreshold = -1368.0\n",
      "CPU times: user 1.1 s, sys: 184 ms, total: 1.29 s\n",
      "Wall time: 1.16 s\n",
      "mean reward = -158.45200\tthreshold = -3874.0\n",
      "CPU times: user 1.16 s, sys: 43.5 ms, total: 1.21 s\n",
      "Wall time: 1.65 s\n",
      "mean reward = -129.30800\tthreshold = -1245.0\n",
      "CPU times: user 1.04 s, sys: 22.4 ms, total: 1.06 s\n",
      "Wall time: 1.14 s\n",
      "mean reward = -142.20000\tthreshold = -1133.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-7cc72679d3c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time sessions = [generate_session() for i in range(n_samples)]# <generate n_samples sessions>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-441e8072c489>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <pick action from policy (at random with probabilities)>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnew_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 250  # sample this many samples\n",
    "percentile = 50  # take this percent of session with highest rewards\n",
    "smoothing = 0.1  # add this thing to all counts for stability\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    # <generate n_samples sessions>\n",
    "    %time sessions = [generate_session() for i in range(n_samples)]\n",
    "\n",
    "    batch_states, batch_actions, batch_rewards = map(np.array, zip(*sessions))\n",
    "\n",
    "    # batch_states: a list of lists of states in each session\n",
    "    # batch_actions: a list of lists of actions in each session\n",
    "    # batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    # <select percentile of your samples>\n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    elite_states = batch_states[batch_rewards > threshold]\n",
    "    # <select states from sessions where rewards are above threshold>\n",
    "    elite_actions = batch_actions[batch_rewards > threshold]\n",
    "    # <select actions from sessions where rewards are above threshold>\n",
    "\n",
    "    elite_states, elite_actions = map(\n",
    "        np.concatenate, [elite_states, elite_actions])\n",
    "    # hint on task above: use np.percentile and numpy-style indexing\n",
    "\n",
    "    # count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy) + smoothing\n",
    "\n",
    "    # <count all state-action occurences in elite_states and elite_actions>\n",
    "    for j in range(len(elite_states)):\n",
    "        elite_counts[elite_states[j], elite_actions[j]] += 1\n",
    "\n",
    "    policy = elite_counts / elite_counts.sum(axis=1, keepdims=True)\n",
    "    # <normalize over each state to get probabilities>\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\" %\n",
    "          (np.mean(batch_rewards), threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-30 20:10:57,298] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x116ff61d0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1165859e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20, 20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True,  # keep progress between .fit(...) calls\n",
    "                      max_iter=1  # make only 1 iteration on each .fit(...)\n",
    "                      )\n",
    "# initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()] * n_actions, range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "\n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0]\n",
    "\n",
    "        # <sample action with such probabilities>\n",
    "        a = np.random.choice(n_actions, p=probs)\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward = 22.44000\tthreshold = 22.0\n",
      "mean reward = 21.68000\tthreshold = 24.0\n",
      "mean reward = 28.71000\tthreshold = 31.3\n",
      "mean reward = 29.75000\tthreshold = 35.9\n",
      "mean reward = 33.75000\tthreshold = 38.3\n",
      "mean reward = 37.94000\tthreshold = 46.3\n",
      "mean reward = 44.40000\tthreshold = 54.0\n",
      "mean reward = 42.33000\tthreshold = 53.0\n",
      "mean reward = 49.11000\tthreshold = 54.6\n",
      "mean reward = 50.81000\tthreshold = 61.6\n",
      "mean reward = 56.09000\tthreshold = 66.0\n",
      "mean reward = 65.24000\tthreshold = 80.3\n",
      "mean reward = 62.42000\tthreshold = 76.3\n",
      "mean reward = 63.42000\tthreshold = 70.0\n",
      "mean reward = 70.20000\tthreshold = 86.0\n",
      "mean reward = 72.13000\tthreshold = 82.0\n",
      "mean reward = 88.02000\tthreshold = 105.0\n",
      "mean reward = 89.14000\tthreshold = 98.3\n",
      "mean reward = 103.61000\tthreshold = 129.3\n",
      "mean reward = 114.95000\tthreshold = 134.6\n",
      "mean reward = 114.80000\tthreshold = 124.6\n",
      "mean reward = 130.18000\tthreshold = 145.0\n",
      "mean reward = 134.12000\tthreshold = 154.3\n",
      "mean reward = 164.68000\tthreshold = 181.3\n",
      "mean reward = 175.85000\tthreshold = 202.4\n",
      "mean reward = 184.26000\tthreshold = 206.3\n",
      "mean reward = 187.80000\tthreshold = 201.6\n",
      "mean reward = 198.86000\tthreshold = 218.3\n",
      "mean reward = 216.15000\tthreshold = 246.3\n",
      "mean reward = 195.50000\tthreshold = 212.2\n",
      "mean reward = 210.94000\tthreshold = 245.2\n",
      "mean reward = 216.16000\tthreshold = 226.6\n",
      "mean reward = 213.66000\tthreshold = 233.0\n",
      "mean reward = 253.59000\tthreshold = 280.2\n",
      "mean reward = 254.52000\tthreshold = 268.9\n",
      "mean reward = 252.44000\tthreshold = 268.6\n",
      "mean reward = 234.03000\tthreshold = 270.9\n",
      "mean reward = 227.75000\tthreshold = 252.2\n",
      "mean reward = 236.04000\tthreshold = 246.0\n",
      "mean reward = 259.54000\tthreshold = 271.6\n",
      "mean reward = 260.57000\tthreshold = 276.3\n",
      "mean reward = 290.39000\tthreshold = 311.0\n",
      "mean reward = 309.79000\tthreshold = 330.9\n",
      "mean reward = 325.83000\tthreshold = 352.3\n",
      "mean reward = 376.69000\tthreshold = 413.6\n",
      "mean reward = 370.15000\tthreshold = 420.0\n",
      "mean reward = 383.76000\tthreshold = 440.9\n",
      "mean reward = 385.17000\tthreshold = 433.9\n",
      "mean reward = 423.63000\tthreshold = 470.3\n",
      "mean reward = 445.53000\tthreshold = 490.8\n",
      "mean reward = 420.90000\tthreshold = 485.9\n",
      "mean reward = 429.75000\tthreshold = 475.3\n",
      "mean reward = 508.87000\tthreshold = 585.0\n",
      "mean reward = 436.58000\tthreshold = 485.9\n",
      "mean reward = 435.76000\tthreshold = 482.3\n",
      "mean reward = 518.79000\tthreshold = 561.6\n",
      "mean reward = 499.20000\tthreshold = 556.6\n",
      "mean reward = 526.10000\tthreshold = 577.0\n",
      "mean reward = 575.51000\tthreshold = 662.8\n",
      "mean reward = 546.66000\tthreshold = 611.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-dbcdfe4ea01a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-dbcdfe4ea01a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-888b567d2077>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#predict array of action probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# <sample action with such probabilities>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "percentile = 70\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    # generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states, batch_actions, batch_rewards = map(np.array, zip(*sessions))\n",
    "    # batch_states: a list of lists of states in each session\n",
    "    # batch_actions: a list of lists of actions in each session\n",
    "    # batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    # <select percentile of your samples>\n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    elite_states = batch_states[batch_rewards > threshold]\n",
    "    # <select states from sessions where rewards are above threshold>\n",
    "    elite_actions = batch_actions[batch_rewards > threshold]\n",
    "\n",
    "    elite_states, elite_actions = map(\n",
    "        np.concatenate, [elite_states, elite_actions])\n",
    "    # elite_states: a list of states from top games\n",
    "    # elite_actions: a list of actions from top games\n",
    "\n",
    "    # <fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "    agent.fit(elite_states, elite_actions)\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\" %\n",
    "          (np.mean(batch_rewards), threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-30 20:15:59,859] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-30 20:15:59,863] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-01-30 20:15:59,870] Starting new video recorder writing to /Users/libfun/Development/Practical_RL/week1/videos/openaigym.video.1.34349.video000000.mp4\n",
      "[2017-01-30 20:16:03,227] Starting new video recorder writing to /Users/libfun/Development/Practical_RL/week1/videos/openaigym.video.1.34349.video000001.mp4\n",
      "[2017-01-30 20:16:06,517] Starting new video recorder writing to /Users/libfun/Development/Practical_RL/week1/videos/openaigym.video.1.34349.video000008.mp4\n",
      "[2017-01-30 20:16:10,530] Starting new video recorder writing to /Users/libfun/Development/Practical_RL/week1/videos/openaigym.video.1.34349.video000027.mp4\n",
      "[2017-01-30 20:16:14,963] Starting new video recorder writing to /Users/libfun/Development/Practical_RL/week1/videos/openaigym.video.1.34349.video000064.mp4\n",
      "[2017-01-30 20:16:20,728] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/libfun/Development/Practical_RL/week1/videos')\n"
     ]
    }
   ],
   "source": [
    "# finish recording\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env, directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "# unwrap\n",
    "env = env.env.env\n",
    "# upload to gym\n",
    "# gym.upload(\"./videos/\",api_key=\"<your_api_key>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.1.34349.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\" + video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Challenge\n",
    "\n",
    "Now try to solve LunarLander-v2 or MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-02 10:14:25,807] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00450888,  0.93701373, -0.45672774, -0.23999281,  0.00523156,\n",
       "        0.1034556 ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n_actions=4\n"
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\" n_actions=%i\" % (n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(64, 64, 64), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(64, 64, 64),\n",
    "                      activation='relu',\n",
    "                      warm_start=True,  # keep progress between .fit(...) calls\n",
    "                      max_iter=1  # make only 1 iteration on each .fit(...)\n",
    "                      )\n",
    "# initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()] * n_actions, range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "\n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0]\n",
    "\n",
    "        # <sample action with such probabilities>\n",
    "        a = np.random.choice(n_actions, p=probs)\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward = -226.17105\tthreshold = -145.9\n",
      "mean reward = -225.99352\tthreshold = -149.4\n",
      "mean reward = -202.86022\tthreshold = -149.1\n",
      "mean reward = -191.35027\tthreshold = -137.4\n",
      "mean reward = -176.28996\tthreshold = -129.8\n",
      "mean reward = -167.29097\tthreshold = -132.9\n",
      "mean reward = -163.70057\tthreshold = -124.1\n",
      "mean reward = -151.74667\tthreshold = -123.5\n",
      "mean reward = -140.25158\tthreshold = -115.8\n",
      "mean reward = -135.26976\tthreshold = -108.0\n",
      "mean reward = -119.03713\tthreshold = -93.4\n",
      "mean reward = -106.38572\tthreshold = -90.3\n",
      "mean reward = -93.72573\tthreshold = -74.9\n",
      "mean reward = -84.15441\tthreshold = -64.4\n",
      "mean reward = -78.20060\tthreshold = -58.2\n",
      "mean reward = -68.47193\tthreshold = -53.2\n",
      "mean reward = -60.62352\tthreshold = -46.8\n",
      "mean reward = -54.13369\tthreshold = -37.2\n",
      "mean reward = -47.22817\tthreshold = -29.2\n",
      "mean reward = -42.87079\tthreshold = -25.8\n",
      "mean reward = -32.51810\tthreshold = -15.2\n",
      "mean reward = -22.32220\tthreshold = -10.3\n",
      "mean reward = -24.43492\tthreshold = -3.6\n",
      "mean reward = -26.83186\tthreshold = -3.2\n",
      "mean reward = -18.66660\tthreshold = -4.4\n",
      "mean reward = -33.57472\tthreshold = 2.7\n",
      "mean reward = -23.52759\tthreshold = 7.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8719de3abead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8719de3abead>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5543be004ced>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mnew_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# record sessions like you did before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/libfun/Development/gym/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/libfun/Development/gym/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/libfun/Development/gym/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mground_contact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mEndContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/Box2D-2.3.2-py3.6-macosx-10.12-x86_64.egg/Box2D/Box2D.py\u001b[0m in \u001b[0;36m__GetBody\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   5005\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mparent\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5006\u001b[0m         \"\"\"\n\u001b[0;32m-> 5007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_Box2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2Fixture___GetBody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 250\n",
    "percentile = 70\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(30):\n",
    "    # generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states, batch_actions, batch_rewards = map(np.array, zip(*sessions))\n",
    "    # batch_states: a list of lists of states in each session\n",
    "    # batch_actions: a list of lists of actions in each session\n",
    "    # batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    # <select percentile of your samples>\n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    elite_states = batch_states[batch_rewards > threshold]\n",
    "    # <select states from sessions where rewards are above threshold>\n",
    "    elite_actions = batch_actions[batch_rewards > threshold]\n",
    "\n",
    "    elite_states, elite_actions = map(\n",
    "        np.concatenate, [elite_states, elite_actions])\n",
    "    # elite_states: a list of states from top games\n",
    "    # elite_actions: a list of actions from top games\n",
    "\n",
    "    # <fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "    agent.fit(elite_states, elite_actions)\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\" %\n",
    "          (np.mean(batch_rewards), threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-02 10:42:38,243] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n"
     ]
    },
    {
     "ename": "DoubleWrapperError",
     "evalue": "Attempted to double wrap with Wrapper: TimeLimit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDoubleWrapperError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5568769b5aca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# finish recording\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/libfun/Development/gym/gym/wrappers/monitoring.py\u001b[0m in \u001b[0;36mMonitor\u001b[0;34m(env, directory, video_callable, force, resume, write_upon_reset, uid, mode)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# TODO: add duration in seconds also\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     return _Monitor(TimeLimit(env, max_episode_steps=env.spec.timestep_limit), directory, video_callable, force, resume,\n\u001b[0m\u001b[1;32m     51\u001b[0m                     write_upon_reset, uid, mode)\n",
      "\u001b[0;32m/Users/libfun/Development/gym/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, max_episode_seconds, max_episode_steps)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTimeLimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_seconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeLimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_seconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_episode_seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_episode_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/libfun/Development/gym/gym/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_wrapper_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configured\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempted to wrap env %s after .configure() was called.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/libfun/Development/gym/gym/core.py\u001b[0m in \u001b[0;36m_update_wrapper_stack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \"\"\"\n\u001b[1;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapper_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_wrapper_stack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_for_duplicate_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapper_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/libfun/Development/gym/gym/core.py\u001b[0m in \u001b[0;36m_check_for_duplicate_wrappers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;34m\"\"\"Raise an error if there are duplicate wrappers. Can be overwritten by subclasses\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwrapper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapper_stack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleWrapperError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempted to double wrap with Wrapper: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDoubleWrapperError\u001b[0m: Attempted to double wrap with Wrapper: TimeLimit"
     ]
    }
   ],
   "source": [
    "# finish recording\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env, directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "# unwrap\n",
    "env = env.env.env\n",
    "# upload to gym\n",
    "# gym.upload(\"./videos/\",api_key=\"<your_api_key>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\" + video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
